{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Merging ALS Data"
      ],
      "metadata": {
        "id": "U95EYnVJetuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4bIbgNgVcz7",
        "outputId": "f1ced46f-5827-4ed7-8722-c7722f5d2026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5ca683bd4467ac3d0f3539a5db5a361641e8829053085dc84a7373815901d640\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.recommendation import ALS\n"
      ],
      "metadata": {
        "id": "wF6IHmG6e7Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hawaii Data"
      ],
      "metadata": {
        "id": "CaV0Tdl3TOUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10%"
      ],
      "metadata": {
        "id": "ykIwaBbYDlWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "hi_10 = pd.read_parquet(\"/content/hi_10.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "hi_10_spark = spark.createDataFrame(hi_10)\n",
        "\n",
        "# Use StringIndexer to transform business and user columns\n",
        "hi_10_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(hi_10_spark).transform(hi_10_spark)\n",
        "hi_10_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(hi_10_transform).transform(hi_10_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_10, test_10 = hi_10_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_10.write.parquet(\"/train_10.parquet\")\n",
        "test_10.write.parquet(\"/test_10.parquet\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcu9bi1FDpST",
        "outputId": "3c226910-267c-4ed3-e25b-7a0253d91c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_10 = spark.read.parquet(\"/train_10.parquet\")\n",
        "test_10 = spark.read.parquet(\"/test_10.parquet\")"
      ],
      "metadata": {
        "id": "4DCzb--Xh5_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_10)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_10 = train_10 \\\n",
        "    .join(user_factors, train_10[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_10[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_10 = test_10 \\\n",
        "    .join(user_factors, test_10[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_10[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_10.write.parquet(\"/train_final_10.parquet\")\n",
        "test_final_10.write.parquet(\"/test_final_10.parquet\")"
      ],
      "metadata": {
        "id": "yipAH3Jfh9Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20%"
      ],
      "metadata": {
        "id": "Kvu6Lz-XiDpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "hi_20 = pd.read_parquet(\"/content/hi_20.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "hi_20_spark = spark.createDataFrame(hi_20)\n",
        "\n",
        "# Use StringIndexer to transform business and user columns\n",
        "hi_20_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(hi_20_spark).transform(hi_20_spark)\n",
        "hi_20_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(hi_20_transform).transform(hi_20_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_20, test_20 = hi_20_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_20.write.parquet(\"/train_20.parquet\")\n",
        "test_20.write.parquet(\"/test_20.parquet\")"
      ],
      "metadata": {
        "id": "jh2ctYA7iDVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_20 = spark.read.parquet(\"/train_20.parquet\")\n",
        "test_20 = spark.read.parquet(\"/test_20.parquet\")"
      ],
      "metadata": {
        "id": "hsKJDlRrjkyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_20)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_20 = train_20 \\\n",
        "    .join(user_factors, train_20[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_20[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_20 = test_20 \\\n",
        "    .join(user_factors, test_20[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_20[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_20.write.parquet(\"/train_final_20.parquet\")\n",
        "test_final_20.write.parquet(\"/test_final_20.parquet\")\n"
      ],
      "metadata": {
        "id": "ppWaPwOljgnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 50%"
      ],
      "metadata": {
        "id": "f7mANmssiYo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "hi_50 = pd.read_parquet(\"/content/hi_10.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "hi_50_spark = spark.createDataFrame(hi_50)\n",
        "\n",
        "# Use StringIndexer to transform business and user columns\n",
        "hi_50_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(hi_50_spark).transform(hi_50_spark)\n",
        "hi_50_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(hi_50_transform).transform(hi_50_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_50, test_50 = hi_10_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_50.write.parquet(\"/train_50.parquet\")\n",
        "test_50.write.parquet(\"/test_50.parquet\")\n"
      ],
      "metadata": {
        "id": "BkceI-fmijYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_50 = spark.read.parquet(\"/train_50.parquet\")\n",
        "test_50 = spark.read.parquet(\"/test_50.parquet\")\n"
      ],
      "metadata": {
        "id": "jwwKgH-rkJON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_50)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_50 = train_50 \\\n",
        "    .join(user_factors, train_50[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_50[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_50 = test_50 \\\n",
        "    .join(user_factors, test_50[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_50[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_50.write.parquet(\"/train_final_50.parquet\")\n",
        "test_final_50.write.parquet(\"/test_final_50.parquet\")\n"
      ],
      "metadata": {
        "id": "8sisV5tJkKbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 80%"
      ],
      "metadata": {
        "id": "V9DK0nlDiocy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "hi_80 = pd.read_parquet(\"/content/hi_80.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "hi_80_spark = spark.createDataFrame(hi_80)\n",
        "\n",
        "# Use StringIndexer to transform business and user columns\n",
        "hi_80_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(hi_80_spark).transform(hi_80_spark)\n",
        "hi_80_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(hi_80_transform).transform(hi_80_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_80, test_80 = hi_10_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_80.write.parquet(\"/train_80.parquet\")\n",
        "test_80.write.parquet(\"/test_80.parquet\")"
      ],
      "metadata": {
        "id": "iZcjGFODir0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_80 = spark.read.parquet(\"/train_80.parquet\")\n",
        "test_80 = spark.read.parquet(\"/test_80.parquet\")\n"
      ],
      "metadata": {
        "id": "6QlUyKR2kgri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_80)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_80 = train_80 \\\n",
        "    .join(user_factors, train_80[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_80[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_80 = test_80 \\\n",
        "    .join(user_factors, test_80[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_80[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_80.write.parquet(\"/train_final_80.parquet\")\n",
        "test_final_80.write.parquet(\"/test_final_80.parquet\")"
      ],
      "metadata": {
        "id": "OvGUNIMgkgkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 100%"
      ],
      "metadata": {
        "id": "UAO4kpz_iqOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "hi_100 = pd.read_parquet(\"/content/hi_100.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "hi_100_spark = spark.createDataFrame(hi_100)\n",
        "\n",
        "# Use StringIndexer to transform business and user columns\n",
        "hi_100_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(hi_100_spark).transform(hi_100_spark)\n",
        "hi_100_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(hi_100_transform).transform(hi_100_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_100, test_100 = hi_100_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_100.write.parquet(\"/train_100.parquet\")\n",
        "test_100.write.parquet(\"/test_100.parquet\")"
      ],
      "metadata": {
        "id": "N1LNO9bAisDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_100 = spark.read.parquet(\"/train_100.parquet\")\n",
        "test_100 = spark.read.parquet(\"/test_100.parquet\")\n"
      ],
      "metadata": {
        "id": "P2hIgDEukzDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_100)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_100 = train_100 \\\n",
        "    .join(user_factors, train_100[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_100[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_100 = test_100 \\\n",
        "    .join(user_factors, test_100[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_100[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_100.write.parquet(\"/train_final_100.parquet\")\n",
        "test_final_100.write.parquet(\"/test_final_100.parquet\")\n"
      ],
      "metadata": {
        "id": "HiN7utwnkxrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## California Data"
      ],
      "metadata": {
        "id": "AidadK_wTLr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DataFrame from the already loaded parquet file\n",
        "ca_10 = pd.read_parquet(\"/content/ca_10.parquet\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
        "\n",
        "# Convert the sampled pandas DataFrame to Spark DataFrame\n",
        "ca_10_spark = spark.createDataFrame(ca_10)\n",
        "\n",
        "# Use StringIndexer to transform the business and user columns\n",
        "ca_10_transform = StringIndexer(inputCol=\"business\", outputCol=\"business_id\").fit(ca_10_spark).transform(ca_10_spark)\n",
        "ca_10_transform = StringIndexer(inputCol=\"user\", outputCol=\"user_id\").fit(ca_10_transform).transform(ca_10_transform)\n",
        "\n",
        "# Randomly split hi_10_transform into training and testing sets\n",
        "train_10, test_10 = ca_10_transform.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Write the training and testing sets to parquet files\n",
        "train_10.write.parquet(\"/train_10.parquet\")\n",
        "test_10.write.parquet(\"/test_10.parquet\")\n"
      ],
      "metadata": {
        "id": "HymNITUTTQAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training and testing sets from parquet files\n",
        "train_10 = spark.read.parquet(\"/train_10.parquet\")\n",
        "test_10 = spark.read.parquet(\"/test_10.parquet\")\n"
      ],
      "metadata": {
        "id": "sKVCOKFOf7tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "als = ALS(rank=60, maxIter=20, regParam=0.1, userCol=\"user_id\", itemCol=\"business_id\", ratingCol=\"rating_binary\", coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train_10)\n",
        "\n",
        "# Extract ALS user and item factor matrices\n",
        "user_factors = als_model.userFactors\n",
        "item_factors = als_model.itemFactors\n",
        "\n",
        "# Merge user factors and item factors to the training dataset\n",
        "user_factors = user_factors.withColumnRenamed(\"features\", \"user_features\")\n",
        "item_factors = item_factors.withColumnRenamed(\"features\", \"business_features\")\n",
        "\n",
        "train_final_10 = train_10 \\\n",
        "    .join(user_factors, train_10[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, train_10[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Merge user factors and item factors to the test dataset\n",
        "test_final_10 = test_10 \\\n",
        "    .join(user_factors, test_10[\"user_id\"] == user_factors[\"id\"], how='left') \\\n",
        "    .drop(user_factors[\"id\"]) \\\n",
        "    .join(item_factors, test_10[\"business_id\"] == item_factors[\"id\"], how='left') \\\n",
        "    .drop(item_factors[\"id\"])\n",
        "\n",
        "# Save the PySpark DataFrame as Parquet files\n",
        "train_final_10.write.parquet(\"/train_final_10.parquet\")\n",
        "test_final_10.write.parquet(\"/test_final_10.parquet\")"
      ],
      "metadata": {
        "id": "1DfRgbsWgAaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}